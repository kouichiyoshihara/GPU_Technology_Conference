{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# イントロダクション\n",
    "\n",
    "## 目標\n",
    "ChainerRLを用いて、強化学習においてどのようにディープラーニングが利用できるのかを理解する。\n",
    "\n",
    "[Chainer](http://chainer.org/)は、ニューラルネットワークを柔軟かつ直観的にモデリングするための、Pythonベースのフレームワークです。\n",
    "ChainerRLはChainerを土台とした、強化学習の包括的なライブラリです。\n",
    "\n",
    "## 学習内容\n",
    "このハンズオンのノートブックでは、以下について学びます。\n",
    "\n",
    "- 強化学習の基礎的な概念\n",
    "- 強化学習におけるディープラーニングのはたらき\n",
    "- 強化学習タスクにおけるChainerRLの使い方\n",
    "\n",
    "## アジェンダ\n",
    "- Chainerの基礎\n",
    "- 強化学習（Reinforcement Learning）\n",
    "- 従来の強化学習: Q学習（Q-Learning）\n",
    "- ChainerRLの使い方\n",
    "- ChainerRLエージェントを改良する\n",
    "- まとめ\n",
    "\n",
    "## 必要な環境\n",
    "* ChainerRL 0.3.0\n",
    "* Chainer 3.1.0\n",
    "* CuPy 2.1.0.1\n",
    "* OpenAI Gym[classic_control] 0.7.4 \n",
    "* Python 3.5 以降\n",
    "* CUDA8.0 対応の GPU\n",
    "\n",
    "このノートブックは自習またはハンズオンセッションを意図して作られています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Chainerの基礎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備: Chainerのインポート\n",
    "\n",
    "初めに、Chainerと関連モジュールをインポートします。CuPyについては後ほど触れます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Chainer \n",
    "from chainer import Chain, Variable, optimizers, serializers, datasets, training\n",
    "from chainer.training import extensions\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import chainer\n",
    "\n",
    "# Import CuPy - A Numpy equivalent on GPU\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "# Utilities\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備: - CuPyでGPUを計算に使う \n",
    "\n",
    "Chainerの関連モジュールであるCuPyはNumPy互換インタフェースのGPUの為の数値計算ライブラリです。\n",
    "CuPyを用いるとユーザーはハードウェアがCPUかGPUか意識する事なく、共通のコードを動かす事が出来るというメリットがあります。\n",
    "CuPyの効果を実感する為の簡単な実験をしてみましょう。\n",
    "\n",
    "はじめに、処理時間を比較するための関数を定義します。具体的には、1000x1000の大きさの行列を作り、転置し、各要素を2倍にする操作を10000回繰り返す関数です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_xp_performance(xp):\n",
    "    a = xp.arange(1000000).reshape(1000, -1)\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    for i in range(10000):\n",
    "        a = xp.arange(1000000).reshape(1000, -1)\n",
    "        b = a.T * 2\n",
    "    t2 = time.perf_counter()\n",
    "    print(t2 -t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この関数の引数には、NumPyとCuPyのどちらを渡すこともできます。これはCuPyがNumPy互換のインタフェースを提供しているからこそ、可能になることです。それでは、NumPyとCuPyそれぞれの処理時間を計測してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実験: NumPy(CPUを用いた計算)で実行\n",
    "\n",
    "基準となる処理時間を計測するため、NumPyを用いて実験します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_xp_performance(np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実験: CuPy(GPUを用いた計算)を試す\n",
    "\n",
    "次に同じ処理をCuPyで試してみましょう。実行の最初にCUDA関連の初期化処理が走る為立ち上がりに時間がかかりますが、10000回のループ処理全体はNumPyと比べて10倍かそれ以上高速になる事が確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_xp_performance(cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ディープラーニングの学習時には、上記の行列操作のようにGPUを用いる事で高速化可能な処理が多く登場します。ディープラーニングを行う際にCuPyは大変強力なライブラリです。\n",
    "特にレイヤーサイズ(中間層のノード数)やレイヤー数が大きい場合は、GPUを用いる事で大幅な高速化が可能になります。\n",
    "\n",
    "以降のタスクでは、CuPyを用いてディープラーニングの学習を行っていく事にしましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1. パーセプトロンによるMNISTの分類\n",
    "\n",
    "MNISTは、機械学習における分類タスクのベンチマークとなるデータセットです。70000枚の手書き数字画像が含まれており、それぞれに0から9までのラベルが付与されています。与えられた画像が10クラスのどれにあたるかを予測することが、このデータセットにおけるタスクとなります。\n",
    "\n",
    "<img src=\"image/mnist.png\">\n",
    "\n",
    "それぞれのサンプルは28×28のグレースケール画像（784次元のベクトル）で表されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多層パーセプトロン（2-layer） の記述\n",
    "\n",
    "最もシンプルなニューラルネットワークとして、サイズ2の多層パーセプトロン（MLP2）を使用します。\n",
    "これは入力と出力、そしてその間にある隠れユニットで構成されます。\n",
    "それらは重み行列とバイアス項を含んだLinear層（全結合層）に接続されています。\n",
    "隠れユニットの活性化関数は双曲線正接関数（tanh）です。\n",
    "\n",
    "<img src=\"image/mlp_tanh.png\" width=\"600\" >\n",
    "\n",
    "以下でMLP2のクラスを実装します。\n",
    "それぞれの層の形式やサイズは``__init__``メソッドの中で定義していることに注意してください。\n",
    "実際の順伝播計算は``__call__``メソッドの方に記述します。\n",
    "ただし、逆伝播計算の定義は陽に行う必要がありません。Chainerが順伝播計算の計算グラフを記憶しているため、それによって逆伝播計算を行うことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2-layer Multi-Layer Perceptron (MLP)\n",
    "class MLP2(Chain):\n",
    "\n",
    "    # Initialization of layers\n",
    "    def __init__(self):\n",
    "        super(MLP2, self).__init__()\n",
    "        with self.init_scope():\n",
    "            # From 784-dimensional input to hidden unit with 100 nodes\n",
    "            self.l1 = L.Linear(784, 100)\n",
    "            # From hidden unit with 100 nodes to output unit with 10 nodes  (10 classes)\n",
    "            self.l2 = L.Linear(100, 10)\n",
    "\n",
    "    # Forward computation by __call__\n",
    "    def __call__(self, x):\n",
    "        # Forward from x to h1 through activation with tanh function\n",
    "        h1 = F.tanh(self.l1(x))\n",
    "        y = self.l2(h1)                 # Forward from h1to y\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST データセットの読み込みと前処理\n",
    "\n",
    "chainer.datasets.get_mnist()を用いることで、MNISTデータセットをメインメモリに読み込むことができます。\n",
    "\n",
    "MNISTの標準的な問題設定に従い、70000ペアあるデータを学習用の60000ペアとテスト用の10000ペアに分割します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = chainer.datasets.get_mnist()\n",
    "print('Train:', len(train))\n",
    "print('Test:', len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実験の設定\n",
    "\n",
    "これらの変数は、実験の間は不変となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchsize = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験 1.1 - CuPyを用いてシンプルなMLP2を学習する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習の準備\n",
    "学習とテストには、Chainer v1.11.0から導入された[Trainer](http://docs.chainer.org/en/stable/tutorial/basic.html#trainer)が使われています。\n",
    "それには、以下に示す標準的な機械学習ワークフローの後ろの３パートが含まれています。\n",
    "\n",
    "<img src=\"image/ml_flow.png\" width=\"600\">\n",
    "Optimizerはモデルの学習で、誤差逆伝播法によってパラメータ（全結合層における重み行列とバイアス項）の更新をする際に使用されます。\n",
    "Chainerは広く使われている[最適化手法](http://docs.chainer.org/en/stable/reference/optimizers.html#optimizers)（SGD, AdaGrad, RMSProp, Adam, etc...）のほとんどをサポートしています。\n",
    "ここでは、SGDを使うことにします。\n",
    "[L.Classifier](https://github.com/pfnet/chainer/blob/master/chainer/links/model/classifier.py) はニューラルネットワーク（ここではMLP2）を用いた分類モデルを作成するためのラッパーです。\n",
    "L.Classifierでは、デフォルトの誤差関数がsoftmax cross entropyとなっています。\n",
    "\n",
    "まずは、Trainerの初期化の為の関数 prepare_classifierを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_classifier(train,test,batchsize,n_epoch,model,enable_cupy):\n",
    "    log_trigger = 600, 'iteration'\n",
    "    classifier_model = L.Classifier(model)\n",
    "    device = -1\n",
    "    if enable_cupy:\n",
    "        print('(CuPy):True','(n_epoch):',n_epoch,'(batchsize):',batchsize)\n",
    "        model.to_gpu()\n",
    "        chainer.cuda.get_device(0).use()\n",
    "        device = 0\n",
    "    else:\n",
    "        print('(CuPy):False','(n_epoch):',n_epoch,'(batchsize):',batchsize)\n",
    "    optimizer = optimizers.SGD()\n",
    "    optimizer.setup(classifier_model)\n",
    "    train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
    "    test_iter = chainer.iterators.SerialIterator(test, batchsize, repeat=False, shuffle=False)    \n",
    "    updater = training.StandardUpdater(train_iter, optimizer, device=device)\n",
    "    trainer = training.Trainer(updater, (n_epoch, 'epoch'), out='out')\n",
    "    trainer.extend(extensions.dump_graph('main/loss'))\n",
    "    trainer.extend(extensions.Evaluator(test_iter, classifier_model, device=device))\n",
    "    trainer.extend(extensions.LogReport(trigger=log_trigger))\n",
    "    trainer.extend(extensions.PrintReport(\n",
    "        ['epoch', 'iteration', 'main/loss', 'validation/main/loss',\n",
    "         'main/accuracy', 'validation/main/accuracy']), trigger=log_trigger)    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUベースで実行するためにCuPyを使用します。 CuPyを用いる為に、enable_cupyはTrueに設定します。\n",
    "epoch数（全てのデータを使って学習する回数）は2に設定します。\n",
    "prepare_classifier関数を使い、trainerを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epoch=2 # Only 2 epochs\n",
    "enable_cupy = True # Use CuPy\n",
    "model = MLP2() # MLP2 model\n",
    "\n",
    "trainer = prepare_classifier(train,test,batchsize,n_epoch,model,enable_cupy) # prepare trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習とテストの方法\n",
    "次に学習とテストの為の関数 train_and_testを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_test(trainer):\n",
    "    training_start = time.perf_counter()    \n",
    "    trainer.run()\n",
    "    elapsed_time = time.perf_counter() - training_start\n",
    "    print('Elapsed time: %3.3f' % elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テストが終わるまで待機\n",
    "\n",
    "実験を行い、最初の結果を得ましょう。実行が終わったら、各エポック毎のaccuracy値とloss値を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_and_test(trainer)  # Please check accuracy and loss of every epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最初の結果を確認する\n",
    "\n",
    "validation/main/accuracy は0.90以下になっているはずです。\n",
    "悪くはありませんが、改善の余地があります。\n",
    "後で別の設定を試してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可視化ツールをインポートする\n",
    "\n",
    "matplotlibを用いて、計算グラフやMNIST画像を表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import utility and visualization tools\n",
    "import pydot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import Image, display\n",
    "import chainer.computational_graph as cg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算グラフを可視化する方法\n",
    "\n",
    "Chainerは入力から誤差関数までの計算グラフを出力することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_graph():\n",
    "    graph = pydot.graph_from_dot_file('out/cg.dot') # load from .dot file\n",
    "    graph[0].write_png('graph.png')\n",
    "\n",
    "    img = Image('graph.png', width=600, height=600)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP2の計算グラフを可視化する\n",
    "\n",
    "display_graph()を実行することで、有向グラフが表示されます。\n",
    "一番上の３つの楕円は、100枚の画像（784次元のベクトル）、100x784の重み行列、Linear層における長さ100のバイアス項ベクトルに対応しています。\n",
    "\n",
    "100個のノード（ユニット）を有する中間の隠れ層は、tanh活性化関数を介して次のLinear層へと繋がっています。最後の100ベクトルは、SoftmaxCrossEntropy誤差関数により、10クラスに対応する正解ラベル(int32)と比較されます。誤差の値はfloat32で与えられます。\n",
    "\n",
    "このグラフを作成したのち、誤差逆伝播法によって誤差を入力へと伝播させ、モデルパラメータ（Linear層の重み行列とバイアス項）を更新することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像をプロットして予測値を表示する\n",
    "\n",
    "\"Answer\"は正解ラベルを、\"Predict\"はモデルが予測したクラスを意味します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_examples():\n",
    "    %matplotlib inline\n",
    "    plt.figure(figsize=(12,50))\n",
    "    if enable_cupy:\n",
    "        model.to_cpu()\n",
    "    for i in range(45, 105): \n",
    "        x = Variable(np.asarray([test[i][0]]))  # test data\n",
    "        t = Variable(np.asarray([test[i][1]]))  # labels\n",
    "        y = model(x)\n",
    "        prediction = y.data.argmax(axis=1)\n",
    "        example = (test[i][0] * 255).astype(np.int32).reshape(28, 28)\n",
    "        plt.subplot(20, 5, i - 44)\n",
    "        plt.imshow(example, cmap='gray')\n",
    "        plt.title(\"No.{0} / Answer:{1}, Predict:{2}\".format(i, t.data[0], prediction[0]))\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 誤分類の例を確認する\n",
    "\n",
    "ほとんどのサンプルは正しく分類されていますが、間違いもいくつかあります。\n",
    "たとえば、最初の行のNo.46の例は、我々からすれば'1'のようですが、モデルは'3'と分類してしまっているようです。\n",
    "2行目のNo.54の例でも、くずれた形の'6'を'2'に誤分類してしいます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験 1.2 - エポック数を増やす\n",
    "\n",
    "テストデータに対する精度を向上せるため、エポック数を増やしてみましょう。他の条件はそのままです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enable_cupy = True\n",
    "n_epoch=5                 # Increased from 2 to 5\n",
    "model = MLP2()\n",
    "trainer = prepare_classifier(train,test,batchsize,n_epoch,model,enable_cupy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5エポックでの実験\n",
    "\n",
    "先ほどよりも、学習が終わるまでの時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_and_test(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 精度の向上を確認\n",
    "\n",
    "前の実験結果よりも誤差（loss）は小さく、validation/main/accuracyは大きく（0.90以上）なっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 誤分類が解消されたかどうか確認\n",
    "\n",
    "前の実験で誤分類がみられたNo.46 と No.54について、今回は正しく分類されたことでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chainerの特徴 - 容易なデバッグ\n",
    "\n",
    "他のフレームワークでは、モデルのどの部分の定義が間違っているのかを教えてくれないため、複雑なニューラルネットワークのデバッグを行うことが困難です。\n",
    "しかし、Chainerでは順伝播計算での型チェックをサポートしているため、プログラムのデバッグをするかのようにニューラルネットワークのデバッグが行なえます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: an enbugged version of MLP\n",
    "\n",
    "MLP2Wrongクラスには、3つのバグが埋め込んであります。実行しながら、1つずつ修正していきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find four bugs in this model definition\n",
    "class MLP2Wrong(Chain):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP2Wrong, self).__init__()\n",
    "        with self.init_scope():\n",
    "            l1 = L.Linear(748, 100)\n",
    "            l2 = L.Linear(100, 10) \n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = F.tahn(self.l1(x))\n",
    "        y = self.l2(x)\n",
    "        return y\n",
    "    \n",
    "enable_cupy = True\n",
    "n_epoch=5\n",
    "\n",
    "model = MLP2Wrong() # MLP2Wrong\n",
    "trainer = prepare_classifier(train,test,batchsize,n_epoch,model,enable_cupy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スタックトレースを読んでエラーを見つける\n",
    "\n",
    "順伝播計算の過程で、スタックトレースがエラー箇所を指摘してくれます。これは、ChainerのDefine-by-Runアプローチによるものです。Define-by-Runアプローチでは、順伝播計算を行うことにより計算グラフを構築します。\n",
    "\n",
    "4つのバグが修正できたら、MLP2Wrongは前に記述したMLP2の定義と同じになるはずです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_and_test(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験 1.3 - 自分のモデルを作る\n",
    "\n",
    "次はあなたの番です。より高い精度を達成するために、自分でモデルを修正してみましょう。\n",
    "\n",
    "ただエポックを増やすだけでは簡単すぎるので、学習にかける時間として10エポック以内かつ100秒以内に、0.95以上の精度を達成してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 複数のオプションによるモデルの定義\n",
    "\n",
    "より良い精度を目指して、モデルのチューニングを行います。オプションは以下のようにたくさんあります。\n",
    "\n",
    "* epoch数を増やす\n",
    "* ネットワークのノード（ユニット）数を増やす\n",
    "* 層の数を増やす\n",
    "* 様々な活性化関数を用いる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create new Multi-Layer Perceptron (MLP)\n",
    "class MLPNew(Chain):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Add more layers?\n",
    "        super(MLPNew, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(784, 100)  # Increase output node as (784, 200)?\n",
    "            self.l2 = L.Linear(100, 10)  # Add one more layer?\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = F.tanh(self.l1(x))  # Replace F.tanh with F.sigmoid  or F.relu ?\n",
    "        y = self.l2(h1)  # Add one more layer?\n",
    "        return y\n",
    "\n",
    "enable_cupy = True #  Use CuPy for faster training\n",
    "n_epoch = 5 # Add more epochs?\n",
    "model = MLPNew()\n",
    "trainer = prepare_classifier(train,test,batchsize,n_epoch,model,enable_cupy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.95以上の精度を持つモデルを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_and_test(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### これで完璧?\n",
    "\n",
    "0.95を超える精度のもとでは、これら60サンプルの例での誤分類は見当たらないでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最良モデルの見た目を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これであなたは、Chainerでニューラルネットワークを記述し学習する方法を学びました。\n",
    "\n",
    "引き続き、強化学習の基礎について学びます。その後ChainerRLを用いて、強化学習におけるディープラーニングの利用方法についても学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 強化学習（Reinforcement Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1: 問題設定\n",
    "\n",
    "上のMNISTのように、各入力に対し正解となる出力があらかじめ訓練データとして与えられており、それを元に入力から出力へのマッピングを学習するような問題を、教師あり学習と呼びます。\n",
    "\n",
    "強化学習では、教師あり学習とは異なり、正解となる出力は与えられず、代わりにモデルが何らかの出力を行うとそれに対してその良さを表す報酬という数値が与えられます。与えられた報酬を元に、報酬を最大化する出力を推定することが強化学習のゴールとなります。\n",
    "\n",
    "状態（state）を持った環境（environment）と、行動する主体であるエージェントの間の相互作用として記述されます。\n",
    "エージェントは環境の現在の状態$s_t$を観測（observation）することができ、それに応じて行動（action）$a_t$を選択します。環境は、$s_t$と$a_t$に応じて、状態を$s_{t+1}$に変化させるとともに、報酬$r_{t+1}$をエージェントに与えます。\n",
    "\n",
    "<img src=\"image/rl.png\" width=\"200\">\n",
    "\n",
    "エージェントの目標は、将来もらえる報酬の和を最大化するような、状態の観測から行動へのマッピングを獲得することです。\n",
    "\n",
    "以下では簡単のため、行動は離散値、多次元の状態$s_t$は離散値または連続値とします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メインアプローチ :  行動価値関数の近似\n",
    "\n",
    "強化学習においては、方策（policy）の表現方法やエージェント内部の動作メカニズム、最適化手法などについて、複数の戦略が存在します。\n",
    "\n",
    "このハンズオンの大部分では、行動価値関数と呼ばれる関数を近似することを考えます。行動価値関数は、将来のすべてのステップにおける累積報酬の期待値を、状態 $s$ と次の行動 $a$ についての関数 $Q(s, a)$で表したものです。\n",
    "\n",
    "最適な行動価値関数 $Q(s, a)$ の良い近似を学習し、各ステップ $t$で与えられる $s_t$に対してQ値 $Q(s_t, a_t)$ を最大にするような行動 $a_t$ をとることによって、最適な方策が実現します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2: Example - CartPole\n",
    "\n",
    "このセクションでは、CartPoleと呼ばれる制御問題を取り上げ、これをどのように強化学習の問題として扱うかを述べていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備: パッケージのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The typical imports\n",
    "import gym\n",
    "gym.undo_logger_setup()  # Turn off gym's default logger settings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set logger level\n",
    "import logging\n",
    "import JSAnimation\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Imports specifically so we can render outputs in Jupyter.\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 描画方法の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display frames as a gif\n",
    "def display_frames_as_gif(frames):\n",
    "    print('Generating a gif animation...')\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='loop'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 環境の設定 : CartPole\n",
    "\n",
    "古典的な強化学習の事例として、[OpenAI Gym](https://gym.openai.com/)のCartPole-v0を取り上げます。これは、黒い台車と黄色い棒の物理シミュレータで、倒立振子の一種です。\n",
    "\n",
    "<img src=\"image/cartpole.png?\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のCartPole-v0の概要は、[Gym's Wiki page](https://github.com/openai/gym/wiki/CartPole-v0)から引用しています。\n",
    "\n",
    "#### 説明\n",
    "摩擦のない軌道上を動く台車に、棒が取り付けられています。\n",
    "棒が直立した状態からスタートし、台車の速度を上げたり下げたりすることによって、棒が倒れることを防ぐのが目的です。\n",
    "\n",
    "#### 観測 (observation) 可能な状態\n",
    "Type: Box(4)\n",
    "\n",
    "Num | Observation | Min | Max\n",
    "---|---|---|---\n",
    "0 | 台車の位置 | -2.4 | 2.4\n",
    "1 | 台車の速度 | -Inf | Inf\n",
    "2 | 棒の角度 | ~ -41.8&deg; | ~ 41.8&deg;\n",
    "3 | 棒の先端の速度 | -Inf | Inf\n",
    "\n",
    "#### 行動（Actions）\n",
    "Type: Discrete(2)\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | 台車を左に押す\n",
    "1 | 台車を右に押す\n",
    "\n",
    "注: 速度の増加量および減少量は、棒の角度に依存するため固定されていません。これは、台車を動かすのに必要なエネルギーを、棒の重心が増加させるためです。\n",
    "\n",
    "#### エピソードの終了条件 \n",
    "1. 棒の角度の絶対値が20.9より大きい\n",
    "2. 台車の位置の絶対値が2.4より大きい（台車の中心がディスプレイの縁に達する）\n",
    "3. エピソードの長さが200ステップを超える。\n",
    "\n",
    "#### 報酬\n",
    "終了時のステップを含め、1ステップごとに１の報酬が与えられます。\n",
    "\n",
    "#### 初期状態\n",
    "全ての状態は、-0.05から+0.05までの一様乱数で与えられます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gymの環境について\n",
    "\n",
    "Gymの環境は、以下の3つのステップで統一されたインターフェースを有しています。\n",
    "\n",
    "#### 1. 名前を指定して環境を作成\n",
    "```python\n",
    "env = gym.make('ENV_NAME')\n",
    "```\n",
    "#### 2. 環境を初期化\n",
    "\n",
    "```python\n",
    "env.reset()\n",
    "```\n",
    "#### 3. actionを実行し、報酬と次状態を観測\n",
    "```python\n",
    "observation, reward, is_finished, info = env.step(action)\n",
    "```\n",
    "\n",
    "  - observation: 次状態の観測値\n",
    "  - reward: 報酬のスカラー値\n",
    "  - is_finished: 現在の状態が最終状態であるかどうかを示すブール値\n",
    "  - info: 追加的な情報\n",
    "\n",
    "環境との相互作用により、強化学習エージェントは累積報酬を最大化する方策をいかに最適化するかを学習します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 環境のはたらき\n",
    "\n",
    "CartPoleの環境を作成し、初期状態や行動の結果を観測してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create  environment of CartPole-v0\n",
    "env = gym.make('CartPole-v0')\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "observation = env.reset()\n",
    "#env.render(mode='rgb_array', close=True)\n",
    "print('initial observation:', observation)\n",
    "\n",
    "action = env.action_space.sample() # Select random action\n",
    "print('random action:', action)\n",
    "observation, reward, is_finished, info = env.step(action)\n",
    "print('next observation:', observation)\n",
    "print('reward:', reward)\n",
    "print('is_finished:', is_finished)\n",
    "print('info:', info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ランダム行動による実験\n",
    "\n",
    "強化学習エージェントを導入する前に、ランダム行動を繰り返して環境がどのように変化するか確認してみましょう。\n",
    "ここではエピソードを10回繰り返し、観測した状態を画面に記録します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_episodes= 10\n",
    "max_length = 200\n",
    "frames = []\n",
    "for i in range(num_episodes):\n",
    "    observation = env.reset() # Initialize environment with random angle of pole between (±0.05)\n",
    "    for t in range(max_length):\n",
    "        frames.append(env.render(mode = 'rgb_array'))\n",
    "        action = env.action_space.sample() # Select random action = push left or right\n",
    "        observation, reward, is_finished, _ = env.step(action) # Take action and get reward and updated observation\n",
    "        if is_finished:\n",
    "            break\n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エピソードの可視化\n",
    "\n",
    "フレームはgifアニメーションで表示できます。\n",
    "アニメーションが作成されるまで数分かかることがあります。\n",
    "playボタンをクリックしてください。 ランダム行動では20ステップほどで簡単に棒が倒れてしまうのが確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.3: 従来の強化学習 = Q学習（Q-learning）\n",
    "\n",
    "Q学習は最もポピュラーな強化学習手法のうちの１つで、各状態における最適な行動を決定します。\n",
    "\n",
    "各ステップ $t$ では、行動 $a_t$ をとり報酬 $r_{t+1}$ と次状態 $s_{t+1}$を受け取り、Q値 $Q(s_t, a_t)$ を以下の式で更新します。\n",
    "\n",
    "<img src=\"image/q-learning.svg\" width=\"800\">\n",
    "\n",
    "数式の詳細な解説はしませんが、直観的には、”更新前のQ値”と”時間割引を考慮した報酬の総和”の差分に学習率 $\\alpha_t$ をかけてQ値を更新していることになります。\n",
    "\n",
    "Q値はその状態・行動のペアが選ばれるたびに更新され、学習率を適切に減衰させると、将来に渡って常に最適な行動を選択した場合の累積報酬の期待値に収束します。このとき$Q(s,a)$を最大にするような行動$a$が$s$における最適な行動となります。\n",
    "\n",
    "強化学習では、まだよく知らない状態や行動を調べるための「探索」と、すでによさそうだとわかっている状態や行動をより深くに調べるための「利用」のバランスを採る必要があります。このために$\\epsilon$-greedy法がよく使われます。これは、エージェントがランダム行動をとるか、現時点で推定される最良行動をとるかを確率的に決める方法です。\n",
    "\n",
    "Q学習を実行する際の最もシンプルな方法は、ある離散状態と行動の組に対するQ値を、Q-tableという表に保存しておくことです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Q学習エージェントの定義\n",
    "\n",
    "ここでは、観測空間をサイズ20000（10^観測数(4) * アクションの種類(2)）のQ-tableに離散化するQLearningAgentクラスを定義します。Q-tableの各セルは、離散化された状態と可能な動作の組に対する現在のQ値を表します。\n",
    "\n",
    "観測された現在の状態と報酬のもとでact()メソッドを呼び出すことで、エージェントはQ-tableを更新し、次の行動を返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.2\n",
    "        self.discount_factor = 0.95\n",
    "        self.exploration_rate = 0.5\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "\n",
    "        self.n_bins = 9\n",
    "        self.splits = [\n",
    "            # Position\n",
    "            np.linspace(-2.4, 2.4, self.n_bins),\n",
    "            # Velocity\n",
    "            np.linspace(-3.5, 3.5, self.n_bins),\n",
    "            # Angle.\n",
    "            np.linspace(-0.5, 0.5, self.n_bins),\n",
    "            # Tip velocity\n",
    "            np.linspace(-2.0, 2.0, self.n_bins)\n",
    "        ]\n",
    "\n",
    "        # Create Q-Table\n",
    "        self.n_actions = 2\n",
    "        num_states = (self.n_bins+1) ** len(self.splits)\n",
    "        self.q_table = np.zeros(shape=(num_states, self.n_actions))\n",
    "\n",
    "    # Turn the observation into integer state\n",
    "    def set_state(self, observation):\n",
    "        state = 0\n",
    "        for i, column in enumerate(observation):\n",
    "            state += np.digitize(x=column, bins=self.splits[i]) * ((self.n_bins + 1) ** i)\n",
    "        return state\n",
    "\n",
    "    # Select action and update\n",
    "    def act(self, observation, reward=None):\n",
    "        next_state = self.set_state(observation)\n",
    "        \n",
    "        if reward is not None:\n",
    "            # Train mode\n",
    "            if self.state is not None:\n",
    "                # Train by updating Q-Table based on current reward and 'last' action.\n",
    "                self.q_table[self.state, self.action] += self.learning_rate * \\\n",
    "                    (reward + self.discount_factor * max(self.q_table[next_state, :]) - self.q_table[self.state, self.action])\n",
    "            # Exploration or exploitation\n",
    "            do_exploration = (1 - self.exploration_rate) < np.random.uniform(0, 1)\n",
    "            if do_exploration:\n",
    "                #  Exploration\n",
    "                next_action = np.random.randint(0, self.n_actions)\n",
    "            else:\n",
    "                # Exploitation\n",
    "                next_action = np.argmax(self.q_table[next_state])\n",
    "        else:\n",
    "            # Test mode \n",
    "            next_action = np.argmax(self.q_table[next_state])\n",
    "\n",
    "        self.state = next_state\n",
    "        self.action = next_action\n",
    "        return next_action\n",
    "    \n",
    "    # Observe terminal state\n",
    "    def stop_episode(self, observation, reward=None):\n",
    "        if reward is not None:\n",
    "            # Train mode\n",
    "            # Train by updating Q-Table based on current reward and 'last' action.\n",
    "            self.q_table[self.state, self.action] += self.learning_rate * \\\n",
    "                (reward - self.q_table[self.state, self.action])\n",
    "        self.state = None\n",
    "        self.action = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エージェントの学習\n",
    "\n",
    "空のQ-tableからスタートし、200エピソードをかけ、エージェントは$\\epsilon$-greedy戦略を用いたQ学習によって$Q(s_t, a_t)$を学習しようとします。結果のGIF画像から、エージェントが試行錯誤によって徐々に学習しているのがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create agent\n",
    "q_agent = QLearningAgent()\n",
    "\n",
    "num_episodes = 200\n",
    "max_length = 200\n",
    "frames = []\n",
    "n_steps = np.zeros((num_episodes,))\n",
    "for i in range(num_episodes):\n",
    "    observation = env.reset() # Initialize environment with random angle of pole between (±0.05)\n",
    "    reward = 0\n",
    "    n_steps[i] = max_length\n",
    "    for t in range(max_length):\n",
    "        if i % 10 == 0:\n",
    "            frames.append(env.render(mode = 'rgb_array'))\n",
    "        action = q_agent.act(observation, reward) # Select random action = push left or right\n",
    "        observation, reward, is_finished, _ = env.step(action) # Take action and get reward and updated observation\n",
    "        if is_finished:\n",
    "            q_agent.stop_episode(observation, reward)\n",
    "            n_steps[i] = t\n",
    "            break\n",
    "env.render(close=True)\n",
    "print(\"Average step in {} episodes: {}\".format(num_episodes, np.mean(n_steps)))\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-tableの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Q-table size: \", q_agent.q_table.size)\n",
    "print(\"Q-table nonzero count: \", np.count_nonzero(q_agent.q_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習済みエージェントのテスト\n",
    "\n",
    "学習中は探索のために$\\epsilon$-greedy法により一定確率でランダムに行動を選んでいるため、学習中のパフォーマンスはエージェントが発揮できる最高のパフォーマンスとは言えません。強化学習アルゴリズムの評価では、学習中のパフォーマンスとは別に探索をせずにこれまでの経験から最善の行動（Q学習では$Q(s,a)$を最大にする$a$）を選んだ場合のパフォーマンスが評価指標としてしばしば使われます。この方法でエージェントを10エピソード分評価してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_episodes = 10\n",
    "max_length = 200\n",
    "frames = []\n",
    "n_steps = np.zeros((num_episodes,))\n",
    "for i in range(num_episodes):\n",
    "    observation = env.reset() # Initialize environment with random angle of pole between (±0.05)\n",
    "    n_steps[i] = max_length\n",
    "    for t in range(max_length):\n",
    "        frames.append(env.render(mode = 'rgb_array'))\n",
    "        action = q_agent.act(observation) # Select random action = push left or right\n",
    "        observation, _, is_finished, _ = env.step(action) # Take action and get reward and updated observation\n",
    "        if is_finished:\n",
    "            n_steps[i] = t\n",
    "            break\n",
    "print(\"Average step in {} episodes: {}\".format(num_episodes, np.mean(n_steps)))\n",
    "env.render(close=True)\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "おそらく棒を200ステップ維持することはできてないのではないかと思います。学習時のエピソード数（`num_episodes`）を増やす、状態の離散化の粒度（`n_bins`）を変える、などの方法でこれは改善できるので、どこまで改善できるかぜひ試してみてください。\n",
    "\n",
    "CartPoleは強化学習のタスクとしてはかなり単純なものなので、Q-table使ったアルゴリズムも検討の余地があります。しかし、例えば状態の次元数が4から100に増えたとすると、同じ粒度で離散化するとテーブルの大きさは$10^{100}*2$となり現実的ではなくなります。そのような場合には、Q関数をパラメータで表された関数で近似するといった方法が採られます。\n",
    "\n",
    "近年特に高次元の場合に成功を収めているのが、その関数近似にニューラルネットを用いる、深層強化学習と呼ばれる手法です。次の章では、ChainerRLを使って深層強化学習をCartPole、そしてAtari 2600のゲームに適用してみます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ChainerRL クイックスタート\n",
    "\n",
    "ChainerRLには、深層強化学習（Deep Reinforcement Learning）アルゴリズムのChainer実装が含まれています。\n",
    "このチャプターでは、深層強化学習の中でもポピュラーな、以下の２つを使用します。\n",
    "\n",
    "* Deep Q-Network [(Mnih et al., 2015)](http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html)\n",
    "* Double DQN [(Hasselt et al., 2016)](https://arxiv.org/abs/1509.06461)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network (DQN)\n",
    "\n",
    "### 準備: ChainerRLのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import chainerrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChainerRLにおける環境\n",
    "\n",
    "ChainerRLは、深層強化学習を行う様々なアルゴリズムをエージェントとして提供しています。\n",
    "\n",
    "ChainerRLに実装されたエージェントは、先程のCartPoleのように、現在の状態を観測でき、また行動を選択したら次の状態と報酬を観測できるような環境であれば適用することができます。OpenAI Gymの環境だけでなく、似たようなインタフェースを用意すれば独自の環境に適用することも可能です。\n",
    "\n",
    "学習時には、状態と報酬を観測し行動を選択する`act_and_train(observation, reward)`と、エピソード最後の状態と報酬を観測して現在のエピソードを終える`stop_episode_and_train(observation, reward)`の2つのメソッドを用います。どちらのメソッドも適切なタイミングでこれまでの経験を元に内部のモデルを更新します。\n",
    "```python\n",
    "# In training episode\n",
    "while not is_finished:\n",
    "    action = agent.act_and_train(observation, reward)  # Observe both observation and reward\n",
    "    observation, reward, is_finished, info = env.step(action)\n",
    "agent.stop_episode_and_train(observation, reward)  # Observe both observation and reward\n",
    "```\n",
    "\n",
    "テスト中には、状態と観測し今のエージェントから見て最適な行動を選択する`act(observation)`と、現在のエピソードを終える`stop_episode()`の2つのメソッドを用います。モデルを更新することも、学習に影響を与えることもありません。\n",
    "```python\n",
    "# In evaluation episode\n",
    "while not is_finished:\n",
    "    action = agent.act(observation)  # Observe observation only\n",
    "    observation, reward, is_finished, info = env.step(action)\n",
    "agent.stop_episode()  # No observation\n",
    "```\n",
    "\n",
    "これらを用いて、学習用のコードとテスト用のコードをそれぞれ次のように書けます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train agent \n",
    "def train_agent(train_agent, num_episodes=10, max_length=200, render=False):\n",
    "    frames = []\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        reward = 0\n",
    "        is_finished = False\n",
    "        R = 0  # return (sum of rewards)\n",
    "        t = 0  # time step\n",
    "        while not is_finished and t < max_episode_len:\n",
    "            if render and i % 10 == 0:\n",
    "                frames.append(env.render(mode = 'rgb_array'))\n",
    "            action = train_agent.act_and_train(obs, reward)\n",
    "            obs, reward, is_finished, _ = env.step(action)\n",
    "            R += reward\n",
    "            t += 1\n",
    "        if i % 10 == 0:\n",
    "            print('episode:', i,\n",
    "                  'R:', R,\n",
    "                  'statistics:', train_agent.get_statistics())\n",
    "        train_agent.stop_episode_and_train(obs, reward, is_finished)  \n",
    "        \n",
    "    env.render(close=True)\n",
    "    return frames\n",
    "\n",
    "# Test agent \n",
    "def test_agent(test_agent, num_episodes=10, max_length=200, render=True):\n",
    "    frames = []\n",
    "    n_steps = np.zeros((num_episodes,))\n",
    "    for i in range(num_episodes):\n",
    "        observation = env.reset() # Initialize environment with random angle of pole between (±0.05)\n",
    "        n_steps[i] = max_length\n",
    "        for t in range(max_length):\n",
    "            if render:\n",
    "                frames.append(env.render(mode = 'rgb_array'))\n",
    "            action = test_agent.act(observation) # Select random action = push left or right\n",
    "            observation, _, is_finished, _ = env.step(action) # Take action and get reward and updated observation\n",
    "            if is_finished:\n",
    "                n_steps[i] = t\n",
    "                break\n",
    "        test_agent.stop_episode()  \n",
    "    env.render(close=True)\n",
    "    return frames, np.mean(n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN\n",
    "\n",
    "ここでは、深層強化学習の先駆けとなった[DQN (Deep Q-Network)](http://dx.doi.org/10.1038/nature14236)を使用します。\n",
    "\n",
    "DQNを使うためには、Q関数を表現するモデルをニューラルネットとして定義する必要があります。このモデルは、環境からの状態観測を入力とし、各行動に対する累積報酬の期待値の予測値を返すようなモデルです。\n",
    "ChainerRLでは下記のように、`chainer.Chain`としてこのモデルを定義することができます。\n",
    "出力は`chainerrl.action_value.ActionValue`を実装した`chainerrl.action_value.DiscreteActionValue`にラップされることに注意してください。\n",
    "ChainerRLはQ関数の出力をラップすることによって、このような離散行動Q関数と、連続行動に対するQ関数である[NAFs (Normalized Advantage Functions)](https://arxiv.org/abs/1603.00748)を同様に扱うことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QFunction(chainer.Chain):\n",
    "\n",
    "    def __init__(self, obs_size, n_actions, n_hidden_channels=30):\n",
    "        super().__init__(\n",
    "            l0=L.Linear(obs_size, n_hidden_channels),\n",
    "            l1=L.Linear(n_hidden_channels, n_hidden_channels),\n",
    "            l2=L.Linear(n_hidden_channels, n_actions))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.tanh(self.l0(x))\n",
    "        h = F.tanh(self.l1(h))\n",
    "        return chainerrl.action_value.DiscreteActionValue(self.l2(h))\n",
    "\n",
    "obs_size = env.observation_space.low.size\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chainerと同様、`chainer.Optimizer`を使ってモデルのパラメータを更新します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use Adam to optimize q_func. eps=1e-2 is for stability.\n",
    "optimizer = chainer.optimizers.Adam(eps=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQNエージェントを作成するには、パラメータ等をもう少しだけ設定する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the discount factor that discounts future rewards.\n",
    "gamma = 0.95\n",
    "\n",
    "# Use epsilon-greedy for exploration\n",
    "explorer = chainerrl.explorers.ConstantEpsilonGreedy(\n",
    "    epsilon=0.5, random_action_func=env.action_space.sample)\n",
    "\n",
    "# DQN uses Experience Replay.\n",
    "# Specify a replay buffer and its capacity.\n",
    "replay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=10 ** 6) # 経験をためておくバッファー\n",
    "\n",
    "# Since observations from CartPole-v0 is numpy.float64 while\n",
    "# Chainer only accepts numpy.float32 by default, specify\n",
    "# a converter as a feature extractor function phi.\n",
    "phi = lambda x: x.astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、環境と相互作用するDQNエージェントを作成しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_func = QFunction(obs_size, n_actions)\n",
    "optimizer.setup(q_func)\n",
    "dqn_agent = chainerrl.agents.DQN(\n",
    "    q_func, optimizer, replay_buffer, gamma, explorer,\n",
    "    replay_start_size=500, update_interval=1,\n",
    "    target_update_interval=100, phi=phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでエージェントと環境の準備が整いました。\n",
    "強化学習を始めるときです！\n",
    "\n",
    "学習では、`agent.act_and_train`を使って探索行動を選択します。\n",
    "`agent.stop_episode_and_train`はエピソードの終了後に呼び出さなければなりません。\n",
    "学習の統計情報は`agent.get_statistics`で確認することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train_episodes = 200\n",
    "max_episode_len = 200\n",
    "train_agent(dqn_agent, num_train_episodes, max_episode_len)\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注: 学習がうまく進まないまま終了してしまう場合があります。その際はQ関数の作成からやり直してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて、学習が終わりました。\n",
    "エージェントはどのくらい成長したでしょうか？\n",
    "`agent.act`と`agent.stop_episode`を用いてテストができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_test_episodes=10\n",
    "frames, average_step = test_agent(dqn_agent, num_test_episodes, max_episode_len)\n",
    "print(\"Average step in {} episodes: {}\".format(num_test_episodes, average_step))\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストのスコアが十分なら、エージェントを保存して再利用できるようにしておきましょう。エージェントの保存とロードはそれぞれ`agent.save`と`agent.load`を呼び出すだけで行なえます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save an agent to the 'agent' directory\n",
    "dqn_agent.save('agent')\n",
    "\n",
    "# Uncomment to load an agent from the 'agent' directory\n",
    "# dqn_agent.load('agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初めての深層強化学習ミッションはこれで完了です！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験: ChainerRLエージェントの改良\n",
    "\n",
    "ChainerのMLPの例と同様、Q関数のネットワークを修正して、オリジナルのQ関数を作ることが出来ます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QFunctionNew(chainer.Chain):\n",
    "\n",
    "    def __init__(self, obs_size, n_actions, n_hidden_channels=30): # Add more hidden channels?\n",
    "        super().__init__(\n",
    "            l0=L.Linear(obs_size, n_hidden_channels), # Add more layers?\n",
    "            l1=L.Linear(n_hidden_channels, n_hidden_channels),\n",
    "            l2=L.Linear(n_hidden_channels, n_actions))\n",
    "\n",
    "    def __call__(self, x, test=False):\n",
    "        h = F.tanh(self.l0(x)) # User F.sigmoid or F.relu?\n",
    "        h = F.tanh(self.l1(h)) # User F.sigmoid or F.relu?\n",
    "        return chainerrl.action_value.DiscreteActionValue(self.l2(h))\n",
    "\n",
    "new_q_func = QFunctionNew(obs_size, n_actions)\n",
    "optimizer.setup(new_q_func)\n",
    "new_dqn_agent = chainerrl.agents.DQN(\n",
    "    new_q_func, optimizer, replay_buffer, gamma, explorer,\n",
    "    replay_start_size=500, update_interval=1,\n",
    "    target_update_interval=100, phi=phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、新しいQ-関数を学習しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train_episodes = 200\n",
    "max_episode_len = 200\n",
    "train_agent(new_dqn_agent, num_train_episodes, max_episode_len)\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて、テストも行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_test_episodes=10\n",
    "frames, average_step = test_agent(new_dqn_agent, num_test_episodes, max_episode_len)\n",
    "print(\"Average step in {} episodes: {}\".format(num_test_episodes, average_step))\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は、DQNの亜種であるDoubleDQNを使用します。通常のDQNと異なるのは、エージェントの作り方だけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_func_double = QFunctionNew(obs_size, n_actions)\n",
    "optimizer.setup(q_func_double)\n",
    "# Double DQN instead of DQN\n",
    "double_dqn_agent = chainerrl.agents.DoubleDQN(\n",
    "    q_func_double, optimizer, replay_buffer, gamma, explorer,\n",
    "    replay_start_size=500, update_interval=1,\n",
    "    target_update_interval=100, phi=phi)\n",
    "\n",
    "num_train_episodes = 200\n",
    "max_episode_len = 200\n",
    "train_agent(double_dqn_agent, num_train_episodes, max_episode_len)\n",
    "\n",
    "num_test_episodes=10\n",
    "frames, average_step = test_agent(double_dqn_agent, num_test_episodes, max_episode_len)\n",
    "print(\"Average step in {} episodes: {}\".format(num_test_episodes, average_step))\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
